{
 "cells": [
{
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jt-ut/GaiaREU/blob/main/GaiaREU-ML_v2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKOw2SGhvBl-"
   },
   "source": [
    "# Welcome to a Machine Learning Workshop for Astronomy!\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "June 12, 2024  \n",
    "Hosts:  \n",
    "Josh Taylor, Postdoctoral Fellow  \n",
    "Stella Offner, Associate Professor  \n",
    "The University of Texas at Austin, Department of Astronomy\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Workshop Goals\n",
    "\n",
    "Today we will see how modern Machine Learning techniques can be used in Python to address common questions we have about (or ask of) data during the analysis stage of research projects. We will cover:\n",
    "1. **Exploratory Data Analysis**, which is the process of quickly getting a high-level view of your data.\n",
    "2. **Clustering**, which is a methodical way of partitioning a dataset into meaningful subgroups for further analysis.\n",
    "3. **Classification**, which incorporates auxiliary information about our data in order to make predictions from it.\n",
    "\n",
    "The first two tasks are examples of **Unsupervised Machine Learning**, which is the branch of ML concerned with making inferences about data *using only the data at hand*. The last is an example of **Supervised Machine Learning**, as it relies on auxiliary information (i.e., a \"supervisor\") to guide its inferences.\n",
    "\n",
    "\n",
    "## Example Data: Gaia Spectroscopy\n",
    "\n",
    "We have selected a real astronomy dataset to help showcase these ML techniques. These data come from the [Gaia Space Mission](https://www.esa.int/Science_Exploration/Space_Science/Gaia_overview), which is an ongoing in-depth survey of ~2 billion objects in the Milky Way. The [Gaia Data Archive](https://gea.esac.esa.int/archive/) housed by the European Space Agency contains many different types of observations made of Gaia's targets.\n",
    "\n",
    "Today we will work with spectroscopy measurements of a small subset of Gaia targets. Our data are 343 flux measurements across a range of red & blue wavelengths for 3,388 different sources. We curated this subset from a much larger set of spectral data found [here](http://cdn.gea.esac.esa.int/Gaia/gdr3/Spectroscopy/xp_sampled_mean_spectrum/). We have normalized our data such that each source's spectra has unit Euclidean norm when considered as a vector.\n",
    "\n",
    "Before we get started exploring Gaia, we need to ensure all Python packages required for further analysis are available. Run the following block, and ask us for help if any of the installation steps do not complete successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8521,
     "status": "ok",
     "timestamp": 1718109389561,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "75d8Y-1JDmJR",
    "outputId": "3fdb8fd1-31ab-42bd-9b70-9ebacd3576f8"
   },
   "outputs": [],
   "source": [
    "## Install additional Python packages\n",
    "#!pip install pandas numpy matplotlib seaborn scikit-learn yellowbrick\n",
    "\n",
    "## Import required modules\n",
    "# For data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "# Dimensionality Reduction\n",
    "from sklearn.manifold import TSNE\n",
    "# Clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "# Classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "\n",
    "# Set some default plotting parameters\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['figure.titleweight'] = 'bold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilkSsShveQAC"
   },
   "source": [
    "# Exploratory Gaia Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyqLhJri_gnq"
   },
   "source": [
    "With Python setup complete, let's fetch our sample from its web source and do a quick inspection of its size & contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1801,
     "status": "ok",
     "timestamp": 1718109415557,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "jc_MhOntr9-4",
    "outputId": "73a8af96-67ad-4840-ed78-61c3ec2ec63f"
   },
   "outputs": [],
   "source": [
    "## Read data from web\n",
    "# Flux measurements at different wavelengths\n",
    "flux = pd.read_csv('https://raw.githubusercontent.com/jt-ut/GaiaREU/main/data/GaiaREU_flux_trn.csv', header=None)\n",
    "# The list of wavelengths [nanometers] at which flux was measured\n",
    "wvl = pd.read_csv('https://raw.githubusercontent.com/jt-ut/GaiaREU/main/data/GaiaREU_wvl-nanometer.csv', header=None)[0]\n",
    "flux.columns = wvl\n",
    "\n",
    "# Inspect the downloaded data\n",
    "print(flux.shape)\n",
    "print(flux.iloc[0])\n",
    "\n",
    "print(wvl.shape)\n",
    "print(wvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EC0879W8Gquy"
   },
   "source": [
    "Our flux data is arranged into a 3,388 row $\\times$ 343 column ***data matrix***.\n",
    "\n",
    "Each row represents an individual source (emitter of light) in the Gaia archive, and each column holds the flux measurements of that source at the 343 different wavelengths stored in the `wvl` vector, spanning the [330,1050] nanometer range. Our data is actually flux from a model fit (not the actual observation). You can read more about Gaia spectral processing [here](https://www.aanda.org/articles/aa/pdf/2023/06/aa43709-22.pdf).  \n",
    "\n",
    "Almost all statistical and machine learning software packages expect a data matrix arranged as above, with ***observations*** contained in its rows and ***variable measurements*** in its columns.\n",
    "\n",
    "Let's plot some of our spectra just to get an idea of what these 343-dimensional vectors can look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958
    },
    "executionInfo": {
     "elapsed": 2610,
     "status": "ok",
     "timestamp": 1718114110194,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "VcI3X85_w4Pp",
    "outputId": "84f3a4ef-6fe4-47a8-b4ef-3ee6567f9a72"
   },
   "outputs": [],
   "source": [
    "# Plot a few spectra\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "fig.tight_layout()\n",
    "sns.lineplot(x = 'wavelength [nm]', y = 'flux', data = pd.DataFrame({'wavelength [nm]': flux.columns, 'flux': flux.iloc[0]}), drawstyle='steps-pre', ax=axes[0,0]).set(title='Source 1')\n",
    "sns.lineplot(x = 'wavelength [nm]', y = 'flux', data = pd.DataFrame({'wavelength [nm]': flux.columns, 'flux': flux.iloc[99]}), drawstyle='steps-pre', ax=axes[0,1]).set(title='Source 100')\n",
    "sns.lineplot(x = 'wavelength [nm]', y = 'flux', data = pd.DataFrame({'wavelength [nm]': flux.columns, 'flux': flux.iloc[1499]}), drawstyle='steps-pre', ax=axes[1,0]).set(title='Source 1500')\n",
    "sns.lineplot(x = 'wavelength [nm]', y = 'flux', data = pd.DataFrame({'wavelength [nm]': flux.columns, 'flux': flux.iloc[2999]}), drawstyle='steps-pre', ax=axes[1,1]).set(title='Source 3000')\n",
    "fig.subplots_adjust(top=1.5, right=1.5);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UoPtUHd1jOP"
   },
   "source": [
    "What we have just done, plotting actual data vectors in their native space, is called ***data visualiation***. Visualizing your data is probably the easiest and most intuitive part of ***exploratory data analysis***, which allows us to get a crude idea of what's \"going on\" in our data. For example, our plots above indicate that the sources in our sample can have drastically different spectral profiles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV73Q70-1SzM"
   },
   "source": [
    "## Gaia Labels of Variable Star Type\n",
    "\n",
    "Each source in our Gaia dataset has auxiliary information associated with it which we have ignored, until now. This additional info is a label indicating the type of star each source likely is (i.e., its *class*), and was generated by a [previous  analyis of the Gaia archive](https://arxiv.org/abs/2211.17238).\n",
    "\n",
    "Let's download our data labels, called class labels, and inspect them. To start, we'd like to know what types of stars are known to exist in our dataset, and how many of each type we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1718109537692,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "FdisRiMXw9Uv",
    "outputId": "2b480727-51b4-4395-c64e-9e17d5b17ade"
   },
   "outputs": [],
   "source": [
    "## Download Gaia labels\n",
    "class_label = pd.read_csv('https://raw.githubusercontent.com/jt-ut/GaiaREU/main/data/GaiaREU_class-name_trn.csv', header=None)[0]\n",
    "# Ensure we have a label for each source\n",
    "print(\"%d labels for %d flux measurements!\" % (class_label.shape[0], flux.shape[0]))\n",
    "\n",
    "# Produce a frequency table, listing the unique class names and the number of sources belonging to each\n",
    "class_label.value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b46cr13503c1"
   },
   "source": [
    "We can see that every source is labeled by an acronym (e.g., YSO). Let's fetch a list of acronym descriptions to learn what these acronyms mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1718109574246,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "fGWGJM9cxkLq",
    "outputId": "c3f6a16e-885e-448e-996e-eecdf134bcfa"
   },
   "outputs": [],
   "source": [
    "## Download class descriptions\n",
    "class_desc = pd.read_csv('https://raw.githubusercontent.com/jt-ut/GaiaREU/main/data/GaiaREU_class-description.csv', header=0)\n",
    "class_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nO_WTm4rT9ks"
   },
   "source": [
    "Given source labels, we might be interested in the group-wise mean flux for each class. Let's compute and plot those for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 10882,
     "status": "ok",
     "timestamp": 1718125380340,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "FyhIHvT52J_F",
    "outputId": "4b631f40-be1c-4dcc-ed7f-c35482dcdb88"
   },
   "outputs": [],
   "source": [
    "## Compute & plot group means + stdevs\n",
    "\n",
    "# Add the class information to our flux data frame, convert from wide to long format for plotting\n",
    "class_stat_df = pd.melt(pd.concat([flux, pd.Series(class_label, name='class')], axis=1), id_vars='class', var_name=\"wvl\", value_name='flux')\n",
    "class_mean_df = class_stat_df.groupby(['class','wvl']).mean().reset_index()\n",
    "\n",
    "# Seaborn Faceting makes group-wise summaries easy to visualize\n",
    "plt_clstat = sns.FacetGrid(class_stat_df, col='class', col_wrap=5)\n",
    "plt_clstat.map(sns.lineplot, \"wvl\", \"flux\", errorbar=('sd',1)).set(ylabel = 'flux [mean +/- 1sd]', xlabel='wavelength [nm]');\n",
    "plt_clstat.fig.subplots_adjust(top=0.9);\n",
    "plt_clstat.fig.suptitle('Class-wise Statistics of Gaia Training Data');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhS87AJB9LlR"
   },
   "source": [
    "\n",
    "# Unsupervised Learning - Dimensionality Reduction\n",
    "\n",
    "Since we can't realistically visualize the 343-dim spectra of all 3,000+ sources in our sample, we will instead build a *representation* of our data and visualize that. **Dimensionality Reduction** is the process of representing high-dimensional data in a lower dimensional space, usually 2-d or 3-d, for easy visualization. This is also called ***embedding*** our data into a lower dimensional space. It is one type of Unsupervised Machine Learning, which is the branch of ML that seeks to learn structure from our data ***without using any label information***.\n",
    "\n",
    "There are many DR algorithms in statistics and machine learning. We introduce a few of the most commonly used below:\n",
    "\n",
    "### PCA\n",
    "\n",
    "Principal Components Analysis is an older technique from statistics that represents data as a linear combination of the eigenvectors of its covariance (correlation) matrix. DR is achieved by projecting data onto just a few (2 or 3) of these eigenvectors. While the more modern DR methods we discuss next are usually more expressive than PCA, it remains a common tool for data reduction because 1) there are no parameters for the user to worry about, 2) there are mathematical proofs of its optimality for variance explanation (no other DR technique has proofs of this nature) and 3) as a linear transformation of our data, it is very fast.\n",
    "\n",
    "\n",
    "### t-SNE\n",
    "\n",
    "t-distributed Stochastic Neighbor Embedding ([t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)) attempts to place points in low-dimensional space such that similarities among their $k$-nearest high-dimensional neighbors are preserved. $k$ is a user-supplied parameter that t-SNE calls *perplexity*. t-SNE defines point-wise similarity as a (Gaussian) function of distance, and minimizes [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) to bring low-dim point similarites in line with their high-dim counterparts.\n",
    "\n",
    "### UMAP\n",
    "\n",
    "Uniform Manifold Approximation & Projection ([UMAP](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html)) is built upon the same idea as t-SNE, except it defines its high-dim and low-dim similarities differently (but still as a function of distance), and then attempts to find low-dim points which minimize the [Cross-Entropy](https://en.wikipedia.org/wiki/Cross_entropy) between themselves and their high-dim counterparts. UMAP also needs a user-supplied parameter it calls *n_neighbors*, which controls how many point-wise similarities should be preserved in low-dim.\n",
    "\n",
    "**There are many other Dimensionality-Reduction techniques!** (e.g., Self-Organizing Maps, Local Linear Embedding, Spectral Embedding, Sammon's Mapping, ... this list is long!). For now, don't worry about the details of how they work. Just remember that they all try to embed points such that their proximity in low-$d$ somehow represents a similarity of the corresponding high-$d$ points.  \n",
    "\n",
    "Let's perform Dimensionality Reduction on our data with t-SNE and see how each of the above algorithms places points in the 2-d plane. t-SNE requires us to specify its *perplexity* parameter before running (roughly, the number of high-$d$ neighbors that t-SNE attempts to align in 2-$d$ space). The default perplexity in scikit-learn's t-SNE function is 30, which we will use, but pick a value lower than 30 and higher than 30 to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91798,
     "status": "ok",
     "timestamp": 1718110194164,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "__MEZ0WyCadw",
    "outputId": "fab74e7d-7495-47fd-8453-19df7b5e7c4b"
   },
   "outputs": [],
   "source": [
    "## t-SNE, using default perplexity = 30\n",
    "T30 = TSNE(perplexity=30.0, n_components=2, random_state = 123, verbose=1, n_jobs=2, init='pca').fit_transform(flux)\n",
    "T30 = pd.DataFrame(T30, columns = ['x','y'])\n",
    "\n",
    "## Set a \"lo\" (<30) and \"hi\" (>30) value to see how t-SNE changes with different perplexities\n",
    "perp_lo = 5 # something lower then 30\n",
    "perp_hi = 100 # something higher than 30\n",
    "\n",
    "## Re-run t-SNE with these different values\n",
    "Tlo = TSNE(perplexity=perp_lo, n_components=2, random_state = 123, verbose=1, n_jobs=2, init='pca').fit_transform(flux)\n",
    "Tlo = pd.DataFrame(Tlo, columns = ['x','y'])\n",
    "\n",
    "Thi = TSNE(perplexity=perp_hi, n_components=2, random_state = 123, verbose=1, n_jobs=2, init='pca').fit_transform(flux)\n",
    "Thi = pd.DataFrame(Thi, columns = ['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 2072,
     "status": "ok",
     "timestamp": 1718110278861,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "hEj_gtftLzhO",
    "outputId": "29c6ad68-9e2c-48e8-bb11-46a23ea3bc2b"
   },
   "outputs": [],
   "source": [
    "## Visualize the PCA, t-SNE, and UMAP representations of our spectra\n",
    "plt.rcParams['figure.figsize'] = (16,6)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\"2-d t-SNE Embedding\", fontsize=14)\n",
    "sns.scatterplot(x='x', y='y', data = Tlo, s=10, ax=axes[0]).set(title=\"Perp = %d\" % perp_lo)\n",
    "sns.scatterplot(x='x', y='y', data = T30, s=10, ax=axes[1]).set(title=\"Perp = 30\")\n",
    "sns.scatterplot(x='x', y='y', data = Thi, s=10, ax=axes[2]).set(title=\"Perp = %d\" % perp_hi)\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab-N17iRne1N"
   },
   "source": [
    "**Which of the above embeddings do you think is \"best\"?**\n",
    "\n",
    "This is a bit of a trick question! There are [many purely quantitative ways](https://www.mdpi.com/2504-4990/5/3/56) to assess the quality of dimensionality reduction. These methods generally report how \"faithful\" the embedding is with respect high-$d$ nearest-neighbor relationships -- e.g., whether two points that are next-door neighbors in the embedding are also next-door neighbors in high-$d$. The downside is that the quality assessment methods rarely agree amongst themselves about which embedding is \"best\".\n",
    "\n",
    "If you know something about your data it is usually better to see how organized an embedding is with respect to a known quantity of interest. Since our data our spectra, we might be interested in the spectral peak (highest measured flux in the signal), the location of this peak, and some measure of peak \"width\" (spread).\n",
    "\n",
    "Let's compute these values and project them onto our t-SNE embeddings to see whether t-SNE has produced any (spectrally) meaningful organization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "executionInfo": {
     "elapsed": 2225,
     "status": "ok",
     "timestamp": 1718110412467,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "-7vpKOnYj63k",
    "outputId": "3d37f7c7-9d30-4a6e-d6af-457e2aee0267"
   },
   "outputs": [],
   "source": [
    "## Compute max(Flux) in each source's spectrum\n",
    "PeakFlux = np.log10(flux.max(axis=1))\n",
    "\n",
    "## Plot this value on the t-SNE Embedding\n",
    "norm = plt.Normalize(PeakFlux.min(), PeakFlux.max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"rocket\", norm=norm)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\"Source Peak log(Flux) Projected on t-SNE Embedding\", fontsize=14);\n",
    "sns.scatterplot(x='x', y='y', data = Tlo, s=10, ax=axes[0], hue=PeakFlux, palette=\"rocket\", legend=False).set(title=\"Perp = %d\" % perp_lo);\n",
    "sns.scatterplot(x='x', y='y', data = T30, s=10, ax=axes[1], hue=PeakFlux, palette=\"rocket\", legend=False).set(title=\"Perp = 30\");\n",
    "sns.scatterplot(x='x', y='y', data = Thi, s=10, ax=axes[2], hue=PeakFlux, palette=\"rocket\", legend=False).set(title=\"Perp = %d\" % perp_hi);\n",
    "\n",
    "fig.colorbar(sm, ax=plt.gca());\n",
    "fig.subplots_adjust(top=0.90);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    },
    "executionInfo": {
     "elapsed": 2234,
     "status": "ok",
     "timestamp": 1718110428417,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "GUw-KQ_Ab7sF",
    "outputId": "92afd754-12ba-4913-ab17-2e69abf21de4"
   },
   "outputs": [],
   "source": [
    "## Compute location of peak in each source's spectrum\n",
    "PeakWvl = flux.idxmax(axis=1)\n",
    "\n",
    "## Plot this value on the t-SNE Embedding\n",
    "norm = plt.Normalize(PeakWvl.min(), PeakWvl.max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu\", norm=norm)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\"Spectra Peak Location [nm] Projected on t-SNE Embedding\", fontsize=14);\n",
    "sns.scatterplot(x='x', y='y', data = Tlo, s=10, ax=axes[0], hue=PeakWvl, palette=\"RdBu\", legend=False).set(title=\"Perp = %d\" % perp_lo);\n",
    "sns.scatterplot(x='x', y='y', data = T30, s=10, ax=axes[1], hue=PeakWvl, palette=\"RdBu\", legend=False).set(title=\"Perp = 30\");\n",
    "sns.scatterplot(x='x', y='y', data = Thi, s=10, ax=axes[2], hue=PeakWvl, palette=\"RdBu\", legend=False).set(title=\"Perp = %d\" % perp_hi);\n",
    "\n",
    "fig.colorbar(sm, ax=plt.gca());\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "executionInfo": {
     "elapsed": 8664,
     "status": "ok",
     "timestamp": 1718110518082,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "IOiFWN4Ws2n0",
    "outputId": "c8bef769-1e84-4b99-c2d1-05d364e5d885"
   },
   "outputs": [],
   "source": [
    "## Define a function to compute a proxy quantity for spectral peak width\n",
    "def spectral_peak_width(f, w):\n",
    "  # f = vector of fluxes, w = wavelengths corresponding to each flux\n",
    "  return np.sqrt(np.sum(f * (w - w.max())**2) / np.sum(f))\n",
    "\n",
    "## Apply the above function to compute peak width for each source\n",
    "PeakSigma = flux.apply(spectral_peak_width, axis=1, w=wvl)\n",
    "\n",
    "## Plot this value on the t-SNE Embedding\n",
    "norm = plt.Normalize(PeakSigma.min(), PeakSigma.max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"mako\", norm=norm)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\"Spectra Peak Width [nm] Projected on t-SNE Embedding\", fontsize=14);\n",
    "sns.scatterplot(x='x', y='y', data = Tlo, s=10, ax=axes[0], hue=PeakSigma, palette=\"mako\", legend=False).set(title=\"Perp = %d\" % perp_lo);\n",
    "sns.scatterplot(x='x', y='y', data = T30, s=10, ax=axes[1], hue=PeakSigma, palette=\"mako\", legend=False).set(title=\"Perp = 30\");\n",
    "sns.scatterplot(x='x', y='y', data = Thi, s=10, ax=axes[2], hue=PeakSigma, palette=\"mako\", legend=False).set(title=\"Perp = %d\" % perp_hi);\n",
    "\n",
    "fig.colorbar(sm, ax=plt.gca());\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfRn_O5pOAx8"
   },
   "source": [
    "**Based on these representations, does our data appear to be completely random, or does it have any meaningful structure?**\n",
    "\n",
    "**What has t-SNE learned about our data?**\n",
    "\n",
    "Even though t-SNE did not see or use our data labels, let's see whether they are organized in the embedding. If you have labels, projecting them to an embedding provides a good sanity-check for determining embedding quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 2051,
     "status": "ok",
     "timestamp": 1718113615673,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "Ge-RI0k3YVc6",
    "outputId": "1798755b-a0b8-4c81-f0e8-0b570bee544e"
   },
   "outputs": [],
   "source": [
    "## Visiualize the class labels of each source on the embeddings\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\"Class Label (Star Type) Projected on t-SNE Embedding\", fontsize=14)\n",
    "sns.scatterplot(x='x', y='y', data = Tlo, s=10, ax=axes[0], hue=class_label, palette=\"Set2\", legend=False).set(title=\"Perp = %d\" % perp_lo);\n",
    "sns.scatterplot(x='x', y='y', data = T30, s=10, ax=axes[1], hue=class_label, palette=\"Set2\", legend=False).set(title=\"Perp = 30\");\n",
    "sns.scatterplot(x='x', y='y', data = Thi, s=10, ax=axes[2], hue=class_label, palette=\"Set2\", legend=True).set(title=\"Perp = %d\" % perp_hi);\n",
    "\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bj5u0YdX1iX"
   },
   "source": [
    "**After visualizing these quantities of interest, Which of the 3 t-SNE embeddings do you prefer, and why?**\n",
    "\n",
    "Store your favorite embedding in the variable TBest below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9ZMhKGSkfk1"
   },
   "outputs": [],
   "source": [
    "## Uncomment the line corresponding to your favorite embedding, and run this cell.\n",
    "## Later, we will use your favorite embedding to visualize more information learned from Gaia\n",
    "#TBest = Tlo\n",
    "#TBest = T30\n",
    "#TBest = Thi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kRhbFVWxXmr"
   },
   "source": [
    "# Unsupervised Learning - Clustering\n",
    "\n",
    "The most common technique in Unsupervised Machine Learning is called [clustering](https://en.wikipedia.org/wiki/Cluster_analysis), which partitions a dataset into $K$ groups of \"similar items.\" This is a more formal process that mimics the visual search for structure we attempted above.\n",
    "\n",
    "As you might suspect, there is no universally applicable definition of what it means for two points to be \"similar\". Because each clustering algorithm differs in how they define point-wise similarities, there is also no universally \"best\" clustering algorithm. Just like in DR analysis, we usually have to try several different algorithms with different parameterizations, and use external knowledge or assumptions about the domain which generated our data (here, astronomy) to pick a good clustering.  \n",
    "\n",
    "## Spectral Clustering\n",
    "\n",
    "Although there are [many](https://scikit-learn.org/stable/modules/clustering.html) algorithms to choose from, we will partition Gaia using  [Spectral Clustering](https://en.wikipedia.org/wiki/Spectral_clustering), which groups data points based on their similarity in a highly customized space that it defines internally. For now, don't worry about the details of how Spectral Clustering identifies clusters.\n",
    "\n",
    "Spectral clustering only has one parameter, $K$, which tells the algorithm how many groups to split the data into. To get a feel for how this works, let's ask Spectral Clustering to partition our data into 10 groups, because we know from above there are 10 distinct class labels, and see how those groups are displayed on the embedding you selected above.  \n",
    "\n",
    "The colors on the resulting plot correspond to each points's cluster label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6347,
     "status": "ok",
     "timestamp": 1718115129916,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "t_yJ_PKO1ldc",
    "outputId": "524884af-b9e6-409f-fa28-6192f358bf32"
   },
   "outputs": [],
   "source": [
    "## Run Spectral Clustering with K=10 groups\n",
    "## The result is an integer vector, length = nrows(flux), which contains the cluster label for each Gaia source\n",
    "clus_SPECTRAL = SpectralClustering(n_clusters=10, assign_labels='discretize', random_state=0).fit(flux).labels_\n",
    "\n",
    "# Plot the clusters labels identified by Spectral Clustering, alongside our known class labels for comparison\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "fig.tight_layout()\n",
    "sns.scatterplot(x='x', y='y', data = TBest, s=10, ax=axes[0], hue=clus_SPECTRAL, palette=\"bright\", legend=True).set(title='Spectral Clustering, K=10');\n",
    "sns.scatterplot(x='x', y='y', data = TBest, s=10, ax=axes[1], hue=class_label, palette=\"Set2\", legend=True).set(title=\"Known Star Type Labels\")\n",
    "plt.show()\n",
    "\n",
    "# Replot the quantites of interest in our spectra, to get an idea what the identified clusters might mean\n",
    "plt.rcParams['figure.figsize'] = (16,6)\n",
    "fig, axes = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "fig.tight_layout()\n",
    "sns.scatterplot(x='x', y='y', data = TBest, s=10, ax=axes[0], hue=PeakFlux, palette=\"rocket\").set(title=\"Peak log(Flux)\")\n",
    "sns.scatterplot(x='x', y='y', data = TBest, s=10, ax=axes[1], hue=PeakWvl, palette=\"RdBu\").set(title=\"Peak Location [nm]\")\n",
    "sns.scatterplot(x='x', y='y', data = TBest, s=10, ax=axes[2], hue=PeakSigma, palette=\"mako\").set(title=\"Peak Spread [nm]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxMGehr-bCco"
   },
   "source": [
    "In the above, we *clustered* the data in high-d space, and visualized the results by *projecting* the cluster labels onto our low-dim embedding.\n",
    "\n",
    "**Do the clusters appear organized on the embedding, with respect to the known class labels?**\n",
    "\n",
    "**Do the clusters appear organized with respect to the quantities of interest we computed above?**\n",
    "\n",
    "**Overall, Does the partition found by Spectral Clustering appear to agree with the structure you see in the embedding?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95XU9WlKou-d"
   },
   "source": [
    "## Selecting the Number Clusters\n",
    "\n",
    "In the above we grouped Gaia point sources into 10 clusters because we know there are 10 distinct class labels. What would you do if you didn't know this information beforehand?\n",
    "\n",
    "This is a challenging task as we usually do not know how many meaningful groups may exist in our data. So how do we decide which $K$ to specify for Spectral Clustering?\n",
    "\n",
    "The typical way of solving this problem is to run the clustering algorithm several times with different values of $K$ and measure the quality of the resulting clusters as reported by various ***clustering performance metrics***. There are [many](https://scikit-learn.org/stable/modules/clustering#clustering-performance-evaluation) such metrics which are inspired by an intuitive notion of what a cluster is: good clusters are generally ***compact***, meaning the points within a cluster are close to each other, and ***well-separated***, meaning the individual clusters are far apart from each other.\n",
    "\n",
    "We will run Spectral Clustering for different $K$ in the range $(2,20)$, and assess the resulting clusterings using three different metrics:\n",
    "* Distortion (measures cluster compactness, low distortion is better)\n",
    "* Silhouette Score (measures tradeoff between compactness & separation, high Silhouette is better)\n",
    "* Calinski-Harabasz Score (measures tradeoff between compactness & separation, high CH is better)\n",
    "\n",
    "We *could* do this exercise manually using functions from scikit-learn, but there is a more automated way of producing & visualizing this information available in Python's yellowbrick package. Below, we produce plots of Distortion/Silhouette/CH score vs. $K$, and inspect the curves to find the $K$ which optimizes each score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "executionInfo": {
     "elapsed": 114921,
     "status": "ok",
     "timestamp": 1718115528032,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "9dwKv8qo8zLB",
    "outputId": "7f00662a-8480-4173-d529-14f85024e7b8"
   },
   "outputs": [],
   "source": [
    "## Spectral Clustering assesses over a range of different K values\n",
    "## First, setup a spectral clustering object as the cluster algorithm, specify the range of different K we will consider\n",
    "model = SpectralClustering(assign_labels='discretize', random_state=0)\n",
    "Krange = (5,20)\n",
    "# Axes to plot different visualizations\n",
    "plt.rcParams['figure.figsize'] = (16,6)\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "# Measure by distortion\n",
    "kselect_dist = KElbowVisualizer(model, k=Krange, metric = 'distortion', timings=False, ax=axes[0])\n",
    "kselect_dist.fit(flux) # Fit data to visualizer\n",
    "kselect_dist.finalize()\n",
    "# Measure by Silhouette\n",
    "kselect_sil = KElbowVisualizer(model, k=Krange, metric = 'silhouette', timings=False, ax=axes[1])\n",
    "kselect_sil.fit(flux) # Fit data to visualizer\n",
    "kselect_sil.finalize()\n",
    "# Measure by Calinski-Harabasz\n",
    "kselect_ch = KElbowVisualizer(model, k=Krange, metric = 'calinski_harabasz', timings=False, ax=axes[2])\n",
    "kselect_ch.fit(flux) # Fit data to visualizer\n",
    "kselect_ch.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAg0t8dZk_MZ"
   },
   "source": [
    "You should see that not all curves have a clearly defined optimal $K$. Selecting the \"right\" number of clusters in a dataset is difficult, and there are often valid arguments to be made supporting *several* candidate $K$s.\n",
    "\n",
    "Let's study the curves and decide whether $K=10$ is a good number of clusters for Gaia, or whether another value might be better. Remember: lower distortion is better, and higher Silhouette & Calinski-Harabasz are better.\n",
    "\n",
    "Based on our discussion, decide how many clusters you believe may exist in Gaia, and store that number of the `Kdesired` variable below. We will re-cluster Gaia using your favorite $K$, and visualize the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "executionInfo": {
     "elapsed": 5287,
     "status": "ok",
     "timestamp": 1718116357621,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "JX0GLqHDXMEx",
    "outputId": "c2bdb858-c47e-46a5-cea4-765cbe8e9a5c"
   },
   "outputs": [],
   "source": [
    "## Set the number of clusters you inferred from the above knee plots in the Kdesired variable\n",
    "myK = 11 # replace ?? with your preferred number of clusters\n",
    "\n",
    "# Run K-Means Clustering to partition Gaia into the desired number of clusters, view alongside known labels\n",
    "clus_myK = SpectralClustering(n_clusters=myK, assign_labels='discretize', random_state=0).fit(flux).labels_\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "fig.tight_layout()\n",
    "sns.scatterplot(x='x', y='y', data = TBest, s=10, ax=axes[0], hue=clus_myK, palette=\"bright\", legend=True).set(title='Spectral Clustering, K=%d' % myK);\n",
    "sns.scatterplot(x='x', y='y', data = TBest, s=10, ax=axes[1], hue=class_label, palette=\"Set2\", legend=True).set(title=\"Known Star Type Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUSGaLON7W6l"
   },
   "source": [
    "Do any of our discovered clusters seem to align with known variable star types? To see, we can compute the mean flux by cluster, and compare to the mean flux by star type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "executionInfo": {
     "elapsed": 3324,
     "status": "ok",
     "timestamp": 1718123798513,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "cskkrsqn6jEp",
    "outputId": "771d0996-0105-4507-bfe7-57361ed5b1c5"
   },
   "outputs": [],
   "source": [
    "## Compute & plot cluster-wise Mean Flux, compare to the (known) class-wise mean fluxes we computed from our labels\n",
    "clus_mean_flux = flux.groupby(clus_myK).mean().reset_index(names=\"clus\").melt(id_vars=\"clus\", var_name=\"wavelength [nm]\", value_name=\"flux\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "fig.tight_layout()\n",
    "sns.lineplot(x = 'wavelength [nm]', y = 'flux', data = clus_mean_flux, drawstyle='steps-pre', hue='clus', palette=\"bright\", ax=axes[0]).set(title=\"Cluster-wise Mean Flux\");\n",
    "sns.lineplot(x = 'wvl', y = 'flux', data = class_mean_df, drawstyle='steps-pre', hue='class', palette=\"Set2\", ax=axes[1]).set(title=\"Class-wise Mean Flux\", xlabel='wavelength [nm]');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWa69zbotVEY"
   },
   "source": [
    "# Supervised Learning with Gaia\n",
    "\n",
    "Previously, we have attempted to learn about Gaia with unsupervised learning techniques. Using only the source spectra, we have created 2-d visualizations of Gaia and determined, via both visual inspection and formal clustering, that there are likely some natural groupings within the data. These techniques are intended to help you explore high-dimensional data when you otherwise know nothing about it.\n",
    "\n",
    "In some settings we are given **auxiliarly information** about our data (i.e., our labels), and are interested in building a model to **predict** this auxiliary info. about new (but similar) data that we may encounter in the future. This task is called Supervised Machine Learning. The \"supervisor\" is the auxiliary info we receive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVxIWQrN3O2x"
   },
   "source": [
    "## Classification with Neural Networks  \n",
    "\n",
    "Given a data vector $x$ (e.g., one of our spectra) and its associated label $y$ (e.g., the star type associated the spectra), Supervised Learning algorithms  try to find a function $f(x)$, which is called the *model*, such that $y = f(x)$ is satisfied for as many $x$ in our dataset as possible. Thus, Supervised Learning is learning the relationship between our data and its label.\n",
    "\n",
    "When $y$ is a categorical label (which is our setting), the process of finding an optimal $f$ is called ***classification***; when $y$ is numeric, it is known as ***regression***.\n",
    "\n",
    "As with clustering, there are [many different classification algorithms](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html). Today, we will fit a [Multi Layer Perceptron](https://scikit-learn.org/stable/modules/neural_networks_supervised.html), also called a neural network. A MLP is a network of artificial nodes (neurons), which are basically small processing units arranged in a user-specified way to connect information in the data to that in the labels.  The process of forming these connections between artificial neurons is called ***neural learning***, or *model fitting*.  The number of neurons, and how they are arranged in the network, are the major parameters a user can control to influence the success of neural learning. Different network architectures give rise to different models from which we must select the best. Generally, more neurons in the network allows better prediction accuracy, but at the cost of longer computational time, and the inability of the model to best generalize its predictions on new data. So, there is a tradeoff.\n",
    "\n",
    "## Model Selection via Cross Validation\n",
    "\n",
    "Once a model $f$ is fit, it can be used to make label predictions $y^* = f(x^*)$ on new (unseen) data $x^*$. In addition to predicting the label $y^*$ for new data, we are also interested in obtaining some idea of how right (or wrong) our predictions might be.  This is called ***uncertainty estimation***, and forms the basis for all statistics.\n",
    "\n",
    "**K-Fold Cross validation** is one way of estimating this uncertainty. The \"best\" supervised learning model among a set of candidate models is then selected as the most accurate, when this uncertainty is incorporated.  Briefly, K-Fold CV involves:\n",
    "1. Splitting our dataset into a training set and a test set.\n",
    "2. The model is fit on the training data, and ...\n",
    "3. used to predict the labels for the data in the test set.\n",
    "4. We then measure the model's predictive accuracy on the test set, because we know the true labels associated with this set.  \n",
    "5. This process is repeated $K$ times, and we infer uncertainty about predictions based on the range of accuracies we observe during the process.\n",
    "\n",
    "Note that in each round of the above, the test data does not influence the model fitting process. This is what allows us to generalize how well our model performs in the real world.\n",
    "\n",
    "## Building a Gaia Classifier\n",
    "\n",
    "Let's specify two different network architectures to learn a predictive model for Gaia. One will be a MLP with 1 hidden layer of 100 neurons, and the other will be a MLP with a hidden layer of 10 neurons. The neurons which are doing the learning are called the network's ***hidden neurons***. We will perform 5-Fold Cross Validation in order to select between the two competing architectures.  \n",
    "\n",
    "There is a lot going on here, but it is relatively easy to accomplish with just a few scikit-learn functions. *Note that the following cell may take a few minutes to run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126624,
     "status": "ok",
     "timestamp": 1718120454298,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "I_iP6VZx4Jqx",
    "outputId": "efce575c-9083-47fd-d169-5d21921ee3b9"
   },
   "outputs": [],
   "source": [
    "## Define model100, with 100 neurons in a single hidden layer\n",
    "model100 = MLPClassifier(hidden_layer_sizes=100, verbose=False, warm_start=True, max_iter=500, tol = 1e-3)\n",
    "## Define model10, with 10 neurons in a single hidden layer\n",
    "model10 = MLPClassifier(hidden_layer_sizes=10, verbose=False, warm_start=True, max_iter=500, tol = 1e-3)\n",
    "\n",
    "## scikit-learn's cross_val_predict function takes a model and a specified number K of CV folds,\n",
    "## splits the whole dataset into train/test sets (folds) K different times.\n",
    "## For each of the K folds, the model is fit on the train set, and labels are predicted for the test set.\n",
    "## The output of cross_val_predict is a label vector, whose i-th component is the prediction made for observation i,\n",
    "## when i was in the test set due to CV splitting.\n",
    "CV_Folds = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "pred_class_label10 = cross_val_predict(model10, X=flux, y=class_label, cv=CV_Folds, n_jobs=5, verbose=2, method='predict')\n",
    "pred_class_label100 = cross_val_predict(model100, X=flux, y=class_label, cv=CV_Folds, n_jobs=5, verbose=2, method='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M7raOzzOV0m"
   },
   "source": [
    "**How do we assess how well our different models performed?**\n",
    "\n",
    "A [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) helps identify where your models are working well, and where they are failing. Confusion matrices are tables whose $(i,j)$ entry contains the count of observations whose true label is $i$, but whose predicted label (by the model) is $j$. Often, these counts are normalized with respect to the total number of observations whose true label is $i$, such that the rowsums of the table = 1. Examining a confusion matrix immediately tells you which classes your classifier is having trouble predicting, if any. Given its special structure, ideally we want the diagonal entries of the matrix to = 1, and all off-diagonal entries = 0.\n",
    "\n",
    "Let's compute and visualize a confusion matrix for the predictions we just made of the star types of Gaia sources with each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "executionInfo": {
     "elapsed": 2267,
     "status": "ok",
     "timestamp": 1718124635813,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "weNHqSZmLH5s",
    "outputId": "5125bb47-6757-4f34-89ef-8b518adc821b"
   },
   "outputs": [],
   "source": [
    "# Make confusion matrices for each model, convert to data frame for plotting\n",
    "cfmat10 = confusion_matrix(y_true = class_label, y_pred = pred_class_label10, normalize='true')\n",
    "cfmat10 = pd.DataFrame(cfmat10, columns=np.unique(class_label), index=np.unique(class_label))\n",
    "\n",
    "cfmat100 = confusion_matrix(y_true = class_label, y_pred = pred_class_label100, normalize='true')\n",
    "cfmat100 = pd.DataFrame(cfmat100, columns=np.unique(class_label), index=np.unique(class_label))\n",
    "\n",
    "# Visualize\n",
    "plt.rcParams['figure.figsize'] = (14,7)\n",
    "fig, axes = plt.subplots(1, 2, sharex=False, sharey=False)\n",
    "fig.tight_layout()\n",
    "fig.suptitle(\"Confusion Matrices for Predicted Class Labels\", fontsize=14)\n",
    "sns.heatmap(cfmat10, cmap = 'Blues', annot=True, fmt='.2f', cbar=False, ax=axes[0]).set(title='10 Neuron Model', xlabel = 'Predicted Class', ylabel = 'True Class');\n",
    "sns.heatmap(cfmat100, cmap = 'Blues', annot=True, fmt='.2f', cbar=False, ax=axes[1]).set(title='100 Neuron Model', xlabel = 'Predicted Class', ylabel = 'True Class');\n",
    "\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMbPtzgaDdow"
   },
   "source": [
    "**Based on these confusion matrices ...**\n",
    "\n",
    "**Do you think the 10 neuron model is better/worse/the same than the 100 neuron model at generalizing its predictions to unseen data (the test set)?**\n",
    "\n",
    "**Which class does each model predict well, and which class does each model predict poorly? Can you guess why? (hint ... look at the class-wise mean flux plots above)**\n",
    "\n",
    "**Which neural network architecture would you use? Store your decisions in the variable best_arch below.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wt9QbLDtDciw"
   },
   "outputs": [],
   "source": [
    "## Uncomment the line below that corresponds to the more accurate model\n",
    "#best_arch = 100\n",
    "#best_arch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CMs4zLIKvuA"
   },
   "source": [
    "**Cross validation is performed solely for model selection purposes.** Once a  model has been selected via CV, we must re-fit it using all available data for training. This gives us the most informed model from which we can make predictions about new data in the future.  Below we re-fit, using the `best_arch` specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 46331,
     "status": "ok",
     "timestamp": 1718120743724,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "NCW47E1w-lE9",
    "outputId": "cccaf526-5a06-4a54-99a5-013be2f33350"
   },
   "outputs": [],
   "source": [
    "## Refit MLP with all data, using selected architecture\n",
    "best_model = MLPClassifier(hidden_layer_sizes=best_arch, verbose=True, max_iter=500, tol = 1e-3)\n",
    "best_model.fit(flux, class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ4QmuCZdh_S"
   },
   "source": [
    "## Predicting Labels for New Gaia Sources\n",
    "\n",
    "Breaking news! An update to the Gaia archive contains spectra for several new sources, and we are asked to predict the labels (type of star) for these new sources based on the classification model we just built. `scikit-learn` makes this relatively easy.\n",
    "\n",
    "Let's fetch & inspect the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1086,
     "status": "ok",
     "timestamp": 1718120754762,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "NfHUpCVfrZA1",
    "outputId": "9030d95b-6180-4e09-cb4e-9eb939dc121b"
   },
   "outputs": [],
   "source": [
    "## Download & inspect new data\n",
    "new_flux = pd.read_csv('https://raw.githubusercontent.com/jt-ut/GaiaREU/main/data/GaiaREU_flux_tst.csv', header=None)\n",
    "new_flux.columns = wvl\n",
    "print(new_flux.shape)\n",
    "print(new_flux.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kv2mqiTWea6w"
   },
   "source": [
    "With a previously fit model in hand, we can easily obtain the predicted labels for these new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1718120766712,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "yIGoA6FpFN95",
    "outputId": "95e510f8-4e6e-412b-db6a-99044cac997f"
   },
   "outputs": [],
   "source": [
    "## Predict class labels for new data using best_model\n",
    "new_flux_predicted_label = best_model.predict(new_flux)\n",
    "pd.Series(new_flux_predicted_label).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n2uhowLCw2e"
   },
   "source": [
    "One simple way to check whether your predictions are good is to determine whether our newly predicted data for a given class look like our known data of that same class. Below, we will plot the spectral statistics for each of our training set classes, and the corresponding stats for these newly predicted classes, for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "executionInfo": {
     "elapsed": 15943,
     "status": "ok",
     "timestamp": 1718126909157,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "Ryziia3OP3AW",
    "outputId": "187ab510-399d-40dc-e7a5-98be6c853619"
   },
   "outputs": [],
   "source": [
    "## Reshape the new_flux data frame for plotting, add in the predicted class label by source\n",
    "tmp = new_flux.reset_index(names='source').melt(id_vars='source',var_name=\"wvl\", value_name=\"flux\")\n",
    "tmp['class'] = new_flux_predicted_label[tmp['source']]\n",
    "tmp.drop(columns='source', inplace=True)\n",
    "tmp.assign(set = \"Test\")\n",
    "\n",
    "combinedf = pd.concat([class_stat_df.assign(set = \"Train\"), tmp.assign(set = \"Test\")])\n",
    "\n",
    "# Seaborn Faceting makes group-wise summaries easy to visualize\n",
    "plt_clstat = sns.FacetGrid(combinedf, col='class', col_wrap=3);\n",
    "plt_clstat.map_dataframe(sns.lineplot, x=\"wvl\", y=\"flux\", errorbar=('sd',1), hue='set').set(ylabel = 'flux [mean +/- 1sd]', xlabel='wavelength [nm]');\n",
    "plt_clstat.fig.subplots_adjust(top=0.9);\n",
    "plt_clstat.fig.suptitle('Class-wise Statistics of Gaia Training & Test-Set Predictions');\n",
    "plt_clstat.add_legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhyqG03VWs4U"
   },
   "source": [
    "**Do the fluxes for the new sources seem to fit with the statistics for the known classes in the training set?**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNrYRe2hxkDvR2AZri5rjfe",
   "provenance": [
    {
     "file_id": "1BCGRQMQQoov5lHfQtnLLH27wNW2Uo_2s",
     "timestamp": 1718128820021
    },
    {
     "file_id": "https://github.com/jt-ut/GaiaREU/blob/main/GaiaREU-ML_v2024.ipynb",
     "timestamp": 1718127355993
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
